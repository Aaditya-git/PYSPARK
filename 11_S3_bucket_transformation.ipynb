{  
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c3dc03-756d-4538-bd02-0dee6d66488b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1209241429508292#setting/sparkui/1117-233950-l3oqwtwq/driver-4570414948628561579\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1209241429508292#setting/sparkui/1117-233950-l3oqwtwq/driver-4570414948628561579\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685e9f8d-c6b0-4771-aba1-93a46e38850f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", \"AKIAUSJEUDG7HD3SYHOI\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "# Read and Transform `portfolio.csv`\n",
    "from pyspark.sql.functions import *\n",
    "# col, split, size, when, regexp_replace, coalesce,concat_ws, to_date, year, current_date\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Unified File Processor\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# S3 Configuration\n",
    "s3_input_bucket = \"my-bucket-tiaa-boto\"  # Input bucket\n",
    "s3_output_bucket = \"my-bucket-tiaa-boto\"  # Output bucket (can be the same or different)\n",
    "input_files = [\"portfolio.csv\", \"profile.csv\",\"transcript.csv\"]  # Input file names\n",
    "output_prefix = \"output-data/\"  # Prefix for processed files\n",
    "\n",
    "raw_Data = \"rawData\"\n",
    "meta_Data = \"metaData\"\n",
    "raw_Data_ = \"rawData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2610fdc5-276e-43f2-81e7-287b5e22dddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "# Read and Transform `portfolio.csv`\n",
    "from pyspark.sql.functions import *\n",
    "# col, split, size, when, regexp_replace, coalesce,concat_ws, to_date, year, current_date\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59de3885-c0db-4e5e-b747-240a16c3745e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Unified File Processor\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# S3 Configuration\n",
    "s3_input_bucket = \"my-bucket-tiaa-boto\"  # Input bucket\n",
    "s3_output_bucket = \"my-bucket-tiaa-boto\"  # Output bucket (can be the same or different)\n",
    "input_files = [\"portfolio.csv\", \"profile.csv\",\"transcript.csv\"]  # Input file names\n",
    "output_prefix = \"output-data/\"  # Prefix for processed files\n",
    "\n",
    "bronze = \"bronze/\"\n",
    "silver_fol = \"silver/\"\n",
    "gold = \"gold/\"\n",
    "\n",
    "raw_Data = \"rawData\"\n",
    "meta_Data = \"metaData\"\n",
    "raw_Data_ = \"rawData/\"\n",
    "\n",
    "\n",
    "def list_s3_files(bucket_name, prefix):\n",
    "\n",
    "# Explicitly pass AWS credentials\n",
    "  s3_client = boto3.client(\n",
    "      \"s3\",\n",
    "      aws_access_key_id=\"AKIAUSJEUDG7HD3SYHOI\",\n",
    "      aws_secret_access_key=\"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\",\n",
    "  )\n",
    "  response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "  if \"Contents\" in response:\n",
    "      return [obj[\"Key\"] for obj in response[\"Contents\"] if obj[\"Key\"].endswith(\".csv\")]\n",
    "  return []\n",
    "\n",
    "\n",
    "def process_portfolio(file_path, output_path):\n",
    "    portfolio_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Drop unnecessary column if present\n",
    "    if \"Unnamed: 0\" in portfolio_df.columns:\n",
    "        portfolio_df = portfolio_df.drop(\"Unnamed: 0\")\n",
    "\n",
    "    # Inspect original data for debugging\n",
    "    portfolio_df = portfolio_df.withColumn(\n",
    "        \"channels\", split(regexp_replace(col(\"channels\"), \"[\\\\[\\\\]' ]\", \"\"), \",\")\n",
    "    )\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_mobile\", lit(False))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_email\", lit(False))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_social\", lit(False))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_web\", lit(False))\n",
    "    \n",
    "    # Set to True if the channel exists in the array\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_mobile\", array_contains(col(\"channels\"), \"mobile\"))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_email\", array_contains(col(\"channels\"), \"email\"))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_social\", array_contains(col(\"channels\"), \"social\"))\n",
    "    portfolio_df = portfolio_df.withColumn(\"channel_web\", array_contains(col(\"channels\"), \"web\"))\n",
    "    \n",
    "    # Drop the original channels column if no longer needed\n",
    "    portfolio_df = portfolio_df.drop(\"channels\")\n",
    "\n",
    "    # Transformations\n",
    "    portfolio_df = portfolio_df.withColumn(\"offer_type\", \n",
    "                                           when(col(\"offer_type\") == \"bogo\", \"buy_one_get_one\")\n",
    "                                           .otherwise(col(\"offer_type\")))\n",
    "    portfolio_df = portfolio_df.withColumn(\"reward\", col(\"reward\").cast(\"double\"))\n",
    "    portfolio_df = portfolio_df.withColumn(\"difficulty\", col(\"difficulty\").cast(\"integer\"))\n",
    "    portfolio_df = portfolio_df.withColumn(\"duration\", col(\"duration\").cast(\"integer\"))\n",
    "    portfolio_df = portfolio_df.na.fill({\"reward\": 0, \"difficulty\": 0, \"duration\": 0})\n",
    "\n",
    "    # Debug transformed data\n",
    "    print(\"After transformations:\")\n",
    "    # portfolio_df.show(5, truncate=False)\n",
    "    portfolio_df = portfolio_df.coalesce(1)\n",
    "    # # Write Transformed Data as CSV\n",
    "    # print(f\"Writing portfolio data to: {output_path}\")\n",
    "    portfolio_df.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "    \n",
    "\n",
    "# Read and Transform `profile.csv`\n",
    "def process_profile(file_path, output_path):\n",
    "    profile_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    #Transformations\n",
    "    profile_df = profile_df.withColumn(\"gender\",\n",
    "                                       when(col(\"gender\").isNull(),lit(\"U\")).otherwise(col(\"gender\")))\n",
    "    profile_df = profile_df.withColumn(\"age\", col(\"age\").cast(\"integer\"))\n",
    "    profile_df = profile_df.withColumn(\"income\", col(\"income\").cast(\"double\"))\n",
    "    profile_df = profile_df.withColumn(\"became_member_on\", to_date(col(\"became_member_on\"), \"yyyyMMdd\"))\n",
    "    profile_df = profile_df.withColumn(\"membership_years\", year(current_date()) - year(col(\"became_member_on\")))\n",
    "    # Write Transformed Data as CSV\n",
    "  \n",
    "    # Write Transformed Data as CSV with fixed name\n",
    "    profile_df.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "# Read and Transform `transcripts.csv`\n",
    "def process_transcript(file_path, output_path):\n",
    "    # Read the input CSV file\n",
    "    transcript_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Define the schema for the JSON 'value' column\n",
    "    value_schema = StructType([\n",
    "        StructField(\"offer id\", StringType(), True),\n",
    "        StructField(\"amount\", FloatType(), True),\n",
    "        StructField(\"reward\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Extract the 'offer_id', 'amount', and 'reward' fields from the 'value' column\n",
    "    transcript_df = transcript_df.withColumn(\"value_json\", from_json(col(\"value\"), value_schema))\n",
    "\n",
    "    # Transformations to extract JSON fields into separate columns\n",
    "    transcript_df = transcript_df.withColumn(\"offer_id\", when(col(\"value_json\").isNotNull() & col(\"value_json\")[\"offer id\"].isNotNull(), col(\"value_json\")[\"offer id\"]).otherwise(lit(0)))\n",
    "    transcript_df = transcript_df.withColumn(\"amount\", when(col(\"value_json\").isNotNull() & col(\"value_json\")[\"amount\"].isNotNull(), col(\"value_json\")[\"amount\"]).otherwise(lit(0)))\n",
    "    transcript_df = transcript_df.withColumn(\"reward\", when(col(\"value_json\").isNotNull() & col(\"value_json\")[\"reward\"].isNotNull(), col(\"value_json\")[\"reward\"]).otherwise(lit(0)))\n",
    "\n",
    "    # Drop the original 'value' and 'value_json' columns, as we have extracted the necessary information\n",
    "    transcript_df = transcript_df.drop(\"value\", \"value_json\")\n",
    "\n",
    "    # Ensure that columns have the appropriate types\n",
    "    transcript_df = transcript_df.withColumn(\"offer_id\", col(\"offer_id\").cast(StringType()))\n",
    "    transcript_df = transcript_df.withColumn(\"amount\", col(\"amount\").cast(FloatType()))\n",
    "    transcript_df = transcript_df.withColumn(\"reward\", col(\"reward\").cast(IntegerType()))\n",
    "\n",
    "    # Write the transformed data as CSV with a fixed name\n",
    "    transcript_df.write.csv(output_path, mode=\"overwrite\", header=True)\n",
    "\n",
    "# Main Processing Logic\n",
    "def process_files():\n",
    "  files = list_s3_files(s3_input_bucket, raw_Data_)\n",
    "  print('myfiles',files)\n",
    "  if not files:\n",
    "    print(\"No files to process.\")\n",
    "    return\n",
    "      \n",
    "  # File paths\n",
    "  portfolio_path = f\"s3a://{s3_input_bucket}/{raw_Data}/{input_files[0]}\"\n",
    "  profile_path = f\"s3a://{s3_input_bucket}/{raw_Data}/{input_files[1]}\"\n",
    "  transcripts_path = f\"s3a://{s3_input_bucket}/{raw_Data}/{input_files[2]}\"\n",
    "  portfolio_output_path = f\"s3a://{s3_output_bucket}/{output_prefix}transformed_portfolio\"\n",
    "  profile_output_path = f\"s3a://{s3_output_bucket}/{output_prefix}transformed_profile\"\n",
    "  transcripts_output_path = f\"s3a://{s3_output_bucket}/{output_prefix}transformed_transcripts\"\n",
    "  metadata_output_path = f\"s3a://{s3_output_bucket}/{meta_Data}\"\n",
    "\n",
    "  print(f\"Processing portfolio file: {portfolio_path}\")\n",
    "  process_portfolio(portfolio_path, portfolio_output_path)\n",
    "\n",
    "  print(f\"Processing profile file: {profile_path}\")\n",
    "  process_profile(profile_path, profile_output_path)\n",
    "\n",
    "  print(f\"Processing transcripts file: {transcripts_path}\")\n",
    "  process_transcript(transcripts_path, transcripts_output_path)\n",
    "\n",
    "  print(\"Creating metadata.\")\n",
    "  create_metadata(metadata_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cfc184-98e4-4c1c-a8b0-aa3d561ba772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myfiles ['rawData/portfolio.csv', 'rawData/profile.csv', 'rawData/transcript.csv']\nProcessing portfolio file: s3a://my-bucket-tiaa-boto/rawData/portfolio.csv\nAfter transformations:\nProcessing profile file: s3a://my-bucket-tiaa-boto/rawData/profile.csv\nProcessing transcripts file: s3a://my-bucket-tiaa-boto/rawData/transcript.csv\nCreating metadata.\nCreating metadata...\nWriting metadata to: s3a://my-bucket-tiaa-boto/metaData\nProcessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n",
    "    print(\"Processing complete.\")\n",
    "    # process_and_insert_files()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "S3_bucket_transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
