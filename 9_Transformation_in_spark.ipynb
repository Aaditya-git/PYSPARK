{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a32a8b0-c703-4fea-b29e-a492d1ea6394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2153557465311195#setting/sparkui/1106-232805-empoboex/driver-8440321860649575522\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2153557465311195#setting/sparkui/1106-232805-empoboex/driver-8440321860649575522\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea791356-e027-4ac7-a649-88afdb3d8342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4041ed5f-68f0-48cf-ba5a-4a2f42b08965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_datafm = spark.read \\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferschema\",\"false\")\\\n",
    "    .option(\"mode\",\"FAILFAST\")\\\n",
    "    .load(\"/FileStore/tables/flight_data.csv\")\n",
    "\n",
    "transform_datafm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d32298f-23b0-4918-a24b-7176378d76e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Normal create dataframe attributes'''\n",
    "\n",
    "\n",
    "my_data =[(1,'Aaditya',150000,'Amazon','United States'),(2,'Sru',200000,'Apple','United States'),\n",
    "          (3,'Raj',100000,'FaceBook','India'),(4,'XoXo',300000,'Google','India'),\n",
    "          (5,'Chen',180000,'Meta','China')]\n",
    "my_column_data = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=True),   # nullable column\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", IntegerType(), nullable=True),\n",
    "    StructField(\"Company\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True), \n",
    "\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1e30370-5806-4e30-ad60-998890086310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_first_dataframe = spark.createDataFrame(data=my_data,schema=my_column_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bbaae6-da55-4df6-9125-72c5c4a39998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[22]: ['id', 'name', 'salary', 'Company', 'country']"
     ]
    }
   ],
   "source": [
    "''' This is something new and you should know about this, this is column function which lists column names related with the schema '''\n",
    "\n",
    "my_first_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ea6bd2-ca04-45c4-aa33-493da540395c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Company: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "''' this is more advanced than .column function, it prints datatype of each column'''\n",
    "# while creating the dataframe if you havnt specified if the columns are nullable or not , it will bydefault set the nullable as TRUE.\n",
    "#Then your next question may be that , how to set it to false ? \n",
    "''' So to answer that, you have to set it while creating the columns itself using structType or StructFiled, like : # Define schema with nullable settings\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), nullable=True),   # nullable column\n",
    "    StructField(\"age\", IntegerType(), nullable=False)    # non-nullable column\n",
    "])'''\n",
    "\n",
    "my_first_dataframe.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51e61ea-8d88-431e-ac50-ecbdf5c0bf63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------+-------------+---+\n| id|   name|salary| Company|      country| id|\n+---+-------+------+--------+-------------+---+\n|  1|Aaditya|150000|  Amazon|United States|  1|\n|  2|    Sru|200000|   Apple|United States|  2|\n|  3|    Raj|100000|FaceBook|        India|  3|\n|  4|   XoXo|300000|  Google|        India|  4|\n|  5|   Chen|180000|    Meta|        China|  5|\n+---+-------+------+--------+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "my_first_dataframe.select(\"*\",col('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b884cc4-7167-42dd-9f01-4ba672713d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+--------+-------------+\n|Employee_id|   name|salary| company|      country|\n+-----------+-------+------+--------+-------------+\n|          1|Aaditya|150000|  Amazon|United States|\n|          2|    Sru|200000|   Apple|United States|\n|          3|    Raj|100000|FaceBook|        India|\n|          4|   XoXo|300000|  Google|        India|\n|          5|   Chen|180000|    Meta|        China|\n+-----------+-------+------+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "# Aliasing\n",
    "\n",
    "my_first_dataframe.select(col('id').alias('Employee_id'),\"name\",\"salary\",\"company\",\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7413b3b2-0a7d-4466-9230-b2f8cf062d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------+-------------+------------+\n| id|   name|salary| Company|      country|   Last_name|\n+---+-------+------+--------+-------------+------------+\n|  1|Aaditya|150000|  Amazon|United States|Bhilegaonkar|\n|  2|    Sru|200000|   Apple|United States|Bhilegaonkar|\n|  3|    Raj|100000|FaceBook|        India|Bhilegaonkar|\n|  4|   XoXo|300000|  Google|        India|Bhilegaonkar|\n|  5|   Chen|180000|    Meta|        China|Bhilegaonkar|\n+---+-------+------+--------+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "# Literal --- is used to add a literal (constant) value to a DataFrame, allowing you to include a fixed value in expressions, transformations, or new columns.\n",
    "\n",
    "my_first_dataframe.select('*',lit(\"Bhilegaonkar\").alias(\"Last_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6ce0ac-08c0-40d1-a8d8-91ca79d9cd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------+-------------+\n| id|name|salary|Company|      country|\n+---+----+------+-------+-------------+\n|  2| Sru|200000|  Apple|United States|\n|  4|XoXo|300000| Google|        India|\n|  5|Chen|180000|   Meta|        China|\n+---+----+------+-------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "# FILTER/WHERE --- A where condition basically\n",
    "\n",
    "''' Filtering : \n",
    ".select to show specific columns\n",
    "'''\n",
    "\n",
    "my_first_dataframe.filter(col('salary') > 150000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e104ecfb-f1bd-4c3a-af1b-95eb977fc1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------+-------+\n| id|name|salary|Company|country|\n+---+----+------+-------+-------+\n|  4|XoXo|300000| Google|  India|\n+---+----+------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "''' Filtering with AND or OR:  A where condition basically\n",
    "'''\n",
    "my_first_dataframe.filter((col('country')=='India') & (col('salary') > 150000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70171198-0636-4e10-994f-592a230ed535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+--------+-------------+-------+\n| id|   name|salary| Company|      country|surname|\n+---+-------+------+--------+-------------+-------+\n|  1|Aaditya|150000|  Amazon|United States|  Kumar|\n|  2|    Sru|200000|   Apple|United States|  Kumar|\n|  3|    Raj|100000|FaceBook|        India|  Kumar|\n|  4|   XoXo|300000|  Google|        India|  Kumar|\n|  5|   Chen|180000|    Meta|        China|  Kumar|\n+---+-------+------+--------+-------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "\n",
    "#Add column -- used to add a column in dataframe using \"withColumn\"\n",
    "\n",
    "my_first_dataframe.withColumn(\"surname\",lit(\"Kumar\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecc14e1-c176-4330-be0e-90eb4dccbfc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------+\n|salary| Company|      country|\n+------+--------+-------------+\n|150000|  Amazon|United States|\n|200000|   Apple|United States|\n|100000|FaceBook|        India|\n|300000|  Google|        India|\n|180000|    Meta|        China|\n+------+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "\n",
    "#Remove column -- used to add a column in dataframe using \"withColumn\"\n",
    "\n",
    "my_first_dataframe.drop(\"id\",col(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365c97cb-2941-4e3f-a274-2af477c4005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Company: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "\n",
    "#Casting-- used to add a column in dataframe using \"withColumn\"\n",
    "\n",
    "my_first_dataframe.withColumn(\"id\",col(\"id\").cast(\"String\")).printSchema() # changed from int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d8100f-f980-43de-a318-97b754e19ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- name: integer (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Company: string (nullable = true)\n |-- country: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This is where our transformation starts. First we will begin from selecting basic columns and we will implement :\n",
    "1. Alias\n",
    "2. Literal\n",
    "3. Filter\n",
    "4. Where\n",
    "5. Add column \n",
    "6. Remove column\n",
    "7. Casting\n",
    "'''\n",
    "\n",
    "#Casting-- used to add a column in dataframe using \"withColumn\"\n",
    "\n",
    "my_first_dataframe.withColumn(\"id\",col(\"id\").cast(\"String\"))\\\n",
    "                  .withColumn(\"name\",col(\"name\").cast(\"Integer\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d76fb194-3148-424c-a144-97cb699ef722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "9.Transformation_in_spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
