{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb42241-5a84-492a-9b48-bc5a3a4b9d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1209241429508292#setting/sparkui/1009-005916-43iqlg61/driver-6838487066932438893\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1209241429508292#setting/sparkui/1009-005916-43iqlg61/driver-6838487066932438893\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aff16ad-da76-475a-9ecd-663552463149",
     "showTitle": true,
     "title": "IMPORT NECESSARY FUNCTIONS FOR YOUR SCHEMA!!!!!"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacb5e6d-5d69-4556-bead-fea4b6142685",
     "showTitle": true,
     "title": "DEFINE YOUR SCHEMA USING STRUCT_TYPE & STRUCT_FIELD!!!"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic Syntax to have StructType array and in that use StructField to define each column with its column datatype and if the value is NULLABLE or not !\n",
    "StructField(\"COLUMN NAME\",DATATYPE,BOOL NULLABLE OR NOT!) --- this is just one field of the list !\n",
    "\n",
    "2nd Method -- DDL method -- ddl_my_schema  =  \"column_name datatype, column_name datatype, ......\"\n",
    "'''\n",
    "my_schema = StructType ([\n",
    "            StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "            StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "            StructField(\"count\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "'''\n",
    "Schema using 2nd method\n",
    "'''\n",
    "\n",
    "ddl_schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bc206e-bf2d-480d-8a88-7bb02e34f1bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n|    United States|          Singapore|   25|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "always start with spark.read to read a File Format in this case a CSV. \n",
    "new things I have learnt \n",
    "a) .option(\"inferschema\",\"TRUE/FLASE\") --- it will automatically infer the datatypes of the column unless stated otherwise.\n",
    "b) .option(\"skiprows\",\"count of the skip rows\") -- used to skip n number of rows from the schema or the file. \n",
    "c) .load(\"filepath\") -- See unless and untill you don't specify the file path using .load, you cannot use dataframe.show(\"N\") method as it will not load the \n",
    "                        (here in this case csv) into the dataframe to show the result. \n",
    "d) .schema(\"your schema name\")  --- this is used to read your defined schema .. look above for more details on how to define a schema using 1st method. \n",
    "\n",
    "'''\n",
    "\n",
    "read_a_file_in_spark = spark.read.format(\"csv\")\\\n",
    "                                 .option(\"header\", \"false\")\\\n",
    "                                 .option(\"inferschema\", \"false\")\\\n",
    "                                 .schema(my_schema)\\\n",
    "                                 .option(\"skiprows\",\"2\")\\\n",
    "                                 .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                                 .load(\"/FileStore/tables/flight_data-1.csv\")\n",
    "read_a_file_in_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba3577e7-88f8-4af9-9ac8-c14bc4f3241d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "always start with spark.read to read a File Format in this case a CSV. \n",
    "new things I have learnt \n",
    "a) .option(\"inferschema\",\"TRUE/FLASE\") --- it will automatically infer the datatypes of the column unless stated otherwise.\n",
    "b) .option(\"skiprows\",\"count of the skip rows\") -- used to skip n number of rows from the schema or the file. \n",
    "c) .load(\"filepath\") -- See unless and untill you don't specify the file path using .load, you cannot use dataframe.show(\"N\") method as it will not load the \n",
    "                        (here in this case csv) into the dataframe to show the result. \n",
    "d) .schema(\"your schema name\")  --- this is used to read your defined schema .. look above for more details on how to define a schema using 1st method. \n",
    "'''\n",
    "\n",
    "read_a_file_in_spark = spark.read.format(\"csv\")\\\n",
    "                                 .option(\"header\", \"false\")\\\n",
    "                                 .option(\"inferschema\", \"false\")\\\n",
    "                                 .schema(ddl_schema)\\\n",
    "                                 .option(\"skiprows\",\"2\")\\\n",
    "                                 .option(\"mode\", \"PERMISSIVE\")\\\n",
    "                                 .load(\"/FileStore/tables/flight_data-1.csv\")\n",
    "read_a_file_in_spark.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "user_defined_schema",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
