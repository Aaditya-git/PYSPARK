{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526669e5-8847-4de1-8ae4-bf93632c17ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1209241429508292#setting/sparkui/1010-191730-2q5pm14y/driver-4626867956423206597\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1209241429508292#setting/sparkui/1010-191730-2q5pm14y/driver-4626867956423206597\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09863349-4dd1-4654-8ba1-b6b64e4d28d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is how you read a JSON. Pretty straightforward and similar to how we read CSV, right?\n",
    "'''\n",
    "\n",
    "\n",
    "spark.read.format(\"json\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .load(\"/FileStore/tables/normal_json.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd620fee-9e14-45ca-99fe-06440645d192",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------+--------+------+\n|     _corrupt_record| age|gender|    name|salary|\n+--------------------+----+------+--------+------+\n|\"name\":\"Manish\",\"...|null|  null|    null|  null|\n|                null|  25|  null|  Nikita| 21000|\n|                null|  16|  null|  Pritam| 22000|\n|                null|  35|  null|Prantosh| 25000|\n|                null|  67|     M|  Vikash| 40000|\n+--------------------+----+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we will read a JSON in which we have one field greater in one of the JSON row. For example.\n",
    "{\"name\":\"Aaditya\", age : 24, \"salary\": 20000},\n",
    "{\"name\":\"sru\", age : 23, \"salary\": 20000, \"home\" : \"Pune\"}\n",
    "\n",
    "here SRU has one more field \"HOME\"\n",
    "\n",
    "The JSON used is following: \n",
    "\n",
    "\"name\":\"Manish\",\"age\":20,\"salary\":20000},\n",
    "{\"name\":\"Nikita\",\"age\":25,\"salary\":21000},\n",
    "{\"name\":\"Pritam\",\"age\":16,\"salary\":22000},\n",
    "{\"name\":\"Prantosh\",\"age\":35,\"salary\":25000},\n",
    "{\"name\":\"Vikash\",\"age\":67,\"salary\":40000,\"gender\":\"M\"}\n",
    "\n",
    "Here you can see first row in JSON has the \"{\" missing. Hence what the code did was : it added a column _corrupt_record by default\n",
    "and inserted the whole corrupted record in it.\n",
    "\n",
    "One more thing to notice is that when we have one extra field in json, spark will add extra column in dataframe to handle that extra field.\n",
    "and will set other records to NULL in case other rows does not have that value.\n",
    "'''\n",
    "\n",
    "\n",
    "spark.read.format(\"json\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .load(\"/FileStore/tables/single_file_json_with_extra_fields.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275d1de6-dc9a-40d5-820a-9829c7dc389a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+\n|age|    name|salary|\n+---+--------+------+\n| 20|  Manish| 20000|\n| 25|  Nikita| 21000|\n| 16|  Pritam| 22000|\n| 35|Prantosh| 25000|\n| 67|  Vikash| 40000|\n+---+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORTANT : You need to add .option(\"multiline\",\"true\") to read multiline JSON.\n",
    "THis is the json i used.\n",
    "[\n",
    "{\n",
    "  \"name\": \"Manish\",\n",
    "  \"age\": 20,\n",
    "  \"salary\": 20000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Nikita\",\n",
    "  \"age\": 25,\n",
    "  \"salary\": 21000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Pritam\",\n",
    "  \"age\": 16,\n",
    "  \"salary\": 22000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Prantosh\",\n",
    "  \"age\": 35,\n",
    "  \"salary\": 25000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Vikash\",\n",
    "  \"age\": 67,\n",
    "  \"salary\": 40000\n",
    "}\n",
    "]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "spark.read.format(\"json\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .load(\"/FileStore/tables/multiline_correct_json.json\").show()''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40bd302-9f9c-44a3-9499-469f27c9325d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n|age|  name|salary|\n+---+------+------+\n| 20|Manish| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "\n",
    "JSON used is \n",
    "{\n",
    "  \"name\": \"Manish\",\n",
    "  \"age\": 20,\n",
    "  \"salary\": 20000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Nikita\",\n",
    "  \"age\": 25,\n",
    "  \"salary\": 21000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Pritam\",\n",
    "  \"age\": 16,\n",
    "  \"salary\": 22000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Prantosh\",\n",
    "  \"age\": 35,\n",
    "  \"salary\": 25000\n",
    "},\n",
    "{\n",
    "  \"name\": \"Vikash\",\n",
    "  \"age\": 67,\n",
    "  \"salary\": 40000\n",
    "}\n",
    "\n",
    "here it is not included in \"[]\" and hence it only read first record.\n",
    "\n",
    "'''\n",
    "\n",
    "spark.read.format(\"json\")\\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .load(\"/FileStore/tables/Multi_line_incorrect.json\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5f4da5-8d29-46f7-9364-cfed16cc2ddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "JSON_DATA_READ",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
